### Проектная работа.   
Создание ЦОД для финансовой организации.   
Требование :   
1. На базе современного оборудования   
2. Применнение технологий Vxlan. Mlag.   
3. Порты UPLINK 100 Gbs. Порты DownLink 25 Gbs.    
4. Возможность роста.    

Проект центра обработки данных (ЦОД)   
1. Общая архитектура   
Топология: Spine‑Leaf с оверлеем VXLAN/EVPN.   

Состав:

Spine‑уровень: 8 коммутаторов Arista 7060CX‑32S (модули 100 Гбит/с);

Leaf‑уровень: 16 коммутаторов Arista 7060CX‑32S;

Border Gateway (BorderGW): 4 коммутатора для выхода в WAN/DMZ;

Серверное оборудование: серверы с NIC 25 Гбит/с, подключённые по схеме MLAG.

2. Конфигурация оборудования
Spine (8 шт.):

модель: Arista 7060CX‑32S;

порты: 32 × 100 Гбит/с (QSFP28);

назначение: соединение со всеми Leaf‑коммутаторами;

конфигурация: каждый Spine подключается ко всем 16 Leaf (по 8 линий на Leaf), всего 128 линий от каждого Spine;

технологии: BGP EVPN для маршрутизации VXLAN‑туннелей, ECMP для балансировки трафика.

Leaf (16 шт.):

модель: Arista 7060CX‑32S;

порты:

16 × 100 Гбит/с (QSFP28): 8 портов — Uplink к Spine, 8 портов — резерв/расширение;

64 × 25 Гбит/с (SFP28): 32 порта — Downlink к серверам, 32 порта — резерв/подключение СХД/других Leaf;

назначение: подключение серверов и обеспечение отказоустойчивости через MLAG;

коэффициент переподписки: 1:2 (Downlink : Uplink).

BorderGW (4 шт.):

модели: Arista 7060CX‑32S или аналоги;

назначение: маршрутизация между VXLAN‑доменом ЦОД и внешними сетями (WAN, DMZ, интернет);

функции: Anycast Gateway для VXLAN, BGP с внешними AS, фильтрация трафика, QoS;

подключение: к Spine‑уровню по 100 Гбит/с.

3. Сетевые технологии
VXLAN:

оверлейная сеть поверх IP‑фабрики (L3);

VNI (VXLAN Network Identifier) для изоляции сегментов (до 16 млн сетей);

туннели VXLAN между Leaf‑коммутаторами (VTEP);

EVPN (BGP) для обмена MAC/IP‑адресами между VTEP, устранение flood‑and‑learn.

MLAG:

на Leaf‑уровне: объединение портов двух коммутаторов в один логический линк для серверов;

сервер подключается двумя кабелями к двум Leaf, которые работают как единый коммутатор;

отказоустойчивость: при сбое одного Leaf трафик идёт через второй без прерывания сессии;

LACP для агрегации линков.

ECMP (Equal‑Cost Multi‑Path):

балансировка трафика между несколькими Spine‑коммутаторами;

использование всех доступных путей Uplink для увеличения пропускной способности.

4. Расчёт пропускной способности и переподписки
Leaf‑уровень (на один коммутатор):

Downlink: 32 порта ×25 Гбит/с=800 Гбит/с;

Uplink: 8 портов ×100 Гбит/с=800 Гбит/с;

коэффициент переподписки:  
800
800
​
 =1:1 (фактически лучше 1:2, см. ниже).

Уточнение по переподписке 1:2:

предполагаем, что не все серверы одновременно используют полную полосу;

реальная нагрузка на Downlink ниже пиковой (например, 400 Гбит/с из 800 Гбит/с);

Uplink 800 Гбит/с обеспечивает запас для пиков и резервирования.

Общая пропускная способность фабрики:

Spine‑Leaf: 8 Spine×16 Leaf×8 линий×100 Гбит/с=1024 Тбит/с (агрегированная);

серверный доступ: 16 Leaf×32 порта×25 Гбит/с=12,8 Тбит/с.

5. Схема подключения
[Сервер 1]──25G──┐
                ├──25G──[Leaf 1]──100G──┐
[Сервер 2]──25G──┘                      ├──100G──[Spine 1]
                                    │
[Сервер 3]──25G──┐                  ├──100G──[Spine 2]
                ├──25G──[Leaf 2]──100G──┤
[Сервер 4]──25G──┘                  ├──100G──...──[Spine 8]
                                    │
                                    ├──100G──[BorderGW 1]
                                    └──100G──[BorderGW 2]
Пояснения:

серверы подключены к двум Leaf через MLAG (2 × 25 Гбит/с);

Leaf соединяются с каждым Spine по 8 линиям 100 Гбит/с (ECMP);

BorderGW подключены к Spine для выхода наружу.

6. Отказоустойчивость
MLAG: отказ одного Leaf не влияет на доступность серверов.

ECMP: отказ Spine приводит к перераспределению трафика между оставшимися.

Резервирование линков: все соединения дублированы.

Anycast Gateway: виртуальные IP‑адреса шлюзов на BorderGW для быстрого переключения.

BGP Graceful Restart: сохранение маршрутов при перезагрузке устройств.

7. Возможность роста
Горизонтальное масштабирование:

добавление новых Leaf‑коммутаторов (до 32–64 шт., в зависимости от Spine);

наращивание Spine при увеличении числа Leaf (добавление до 16–32 Spine);

подключение дополнительных BorderGW.

Вертикальное масштабирование:

замена модулей на более скоростные (например, 400 Гбит/с в перспективе);

увеличение числа портов на Leaf за счёт свободных слотов.

Программно‑определяемая сеть (SDN):

внедрение контроллера (Arista CloudVision) для автоматизации настройки VXLAN/EVPN;

оркестрация политик безопасности и QoS через единый интерфейс.

8. Этапы внедрения
Проектирование:

расчёт трафика и переподписки;

разработка L3‑схемы и IP‑плана;

определение VNI и подсетей.

Закупка и монтаж:

поставка оборудования;

установка стоек, прокладка кабелей, маркировка.

Базовая настройка:

IP‑адресация на интерфейсах;

настройка BGP между Spine, Leaf, BorderGW;

включение VXLAN и EVPN.

Настройка MLAG:

создание MLAG‑доменов на парах Leaf;

проверка синхронизации и отказоустойчивости.

Тестирование:

нагрузочные тесты (iperf, mgen);

имитация сбоев (отключение Leaf, Spine, линков);

проверка времени восстановления.

Ввод в эксплуатацию:

подключение серверов;

развёртывание сервисов;

запуск мониторинга.

9. Мониторинг и управление
Инструменты:

Arista CloudVision — централизованное управление и мониторинг;

Prometheus + Grafana — визуализация метрик;

Zabbix — алерты и логирование;

Ansible — автоматизация конфигураций.

Метрики:

загрузка портов (Uplink/Downlink);

ошибки и потери пакетов;

температура и питание оборудования;

состояние BGP‑сессий и V
